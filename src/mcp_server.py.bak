"""
Oracle Database MCP ì„œë²„ ë©”ì¸
15ê°œ Tools ì œê³µ
"""

import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# MCP imports
from mcp.server import Server
from mcp.server.stdio import stdio_server

# ë¡œì»¬ ëª¨ë“ˆ imports
from oracle_connector import OracleConnector
from credentials_manager import CredentialsManager
from csv_parser import CSVParser
from metadata_manager import MetadataManager
from sql_executor import SQLExecutor
from tnsnames_parser import TNSNamesParser
from common_metadata_manager import CommonMetadataManager

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# MCP ì„œë²„ ìƒì„±
server = Server("oracle-nlsql-mcp")

# ì „ì—­ ê°ì²´ë“¤
credentials_manager = CredentialsManager()
csv_parser = CSVParser()
common_metadata_manager = CommonMetadataManager()
metadata_manager = MetadataManager(common_metadata_manager=common_metadata_manager)
tnsnames_parser = TNSNamesParser()

# DB ì»¤ë„¥í„° ìºì‹œ
db_connectors = {}

# tnsnames ìºì‹œ íŒŒì¼ ê²½ë¡œ
TNSNAMES_CACHE_FILE = Path(__file__).parent.parent / "tnsnames_cache.json"


def get_connector(database_sid: str) -> OracleConnector:
    """DB ì»¤ë„¥í„° ê°€ì ¸ì˜¤ê¸° (ìºì‹±)"""
    if database_sid not in db_connectors:
        credentials = credentials_manager.load_credentials(database_sid)

        connector = OracleConnector(
            host=credentials['host'],
            port=credentials['port'],
            service_name=credentials['service_name'],
            user=credentials['user'],
            password=credentials['password']
        )

        connector.connect()
        db_connectors[database_sid] = connector

    return db_connectors[database_sid]


# ============================================
# Tools ëª©ë¡ ë“±ë¡
# ============================================
@server.list_tools()
async def list_tools() -> list:
    """ì‚¬ìš© ê°€ëŠ¥í•œ Tool ëª©ë¡ ë°˜í™˜"""
    import mcp.types as types

    return [
        types.Tool(
            name="register_database_credentials",
            description="DB ì ‘ì† ì •ë³´ë¥¼ ì•”í˜¸í™”í•˜ì—¬ ì €ì¥",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "host": {"type": "string", "description": "í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ"},
                    "port": {"type": "integer", "description": "í¬íŠ¸ ë²ˆí˜¸"},
                    "service_name": {"type": "string", "description": "ì„œë¹„ìŠ¤ ì´ë¦„"},
                    "user": {"type": "string", "description": "ì‚¬ìš©ì ì´ë¦„"},
                    "password": {"type": "string", "description": "ë¹„ë°€ë²ˆí˜¸"}
                },
                "required": ["database_sid", "host", "port", "service_name", "user", "password"]
            }
        ),
        types.Tool(
            name="delete_database",
            description="ë“±ë¡ëœ DB ì‚­ì œ (ì ‘ì† ì •ë³´ ë° ë©”íƒ€ë°ì´í„° ëª¨ë‘ ì‚­ì œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"}
                },
                "required": ["database_sid"]
            }
        ),
        types.Tool(
            name="load_tnsnames",
            description="tnsnames.ora íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ DB ëª©ë¡ ì¶”ì¶œ ë° ìºì‹±",
            inputSchema={
                "type": "object",
                "properties": {
                    "tnsnames_path": {"type": "string", "description": "tnsnames.ora íŒŒì¼ ì „ì²´ ê²½ë¡œ"}
                },
                "required": ["tnsnames_path"]
            }
        ),
        types.Tool(
            name="list_available_databases",
            description="tnsnames.oraì—ì„œ íŒŒì‹±ëœ DB ëª©ë¡ ì¡°íšŒ",
            inputSchema={"type": "object", "properties": {}}
        ),
        types.Tool(
            name="connect_database",
            description="íŠ¹ì • DBì— ì—°ê²° (tnsnamesì—ì„œ í˜¸ìŠ¤íŠ¸/í¬íŠ¸/ì„œë¹„ìŠ¤ëª… ìë™ ë¡œë“œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "user": {"type": "string", "description": "ì‚¬ìš©ì ì´ë¦„"},
                    "password": {"type": "string", "description": "ë¹„ë°€ë²ˆí˜¸"}
                },
                "required": ["database_sid", "user", "password"]
            }
        ),
        types.Tool(
            name="register_common_columns",
            description="ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ë“±ë¡ (DBë³„)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "columns": {"type": "array", "description": "ì¹¼ëŸ¼ ì •ë³´ ë°°ì—´"}
                },
                "required": ["database_sid", "columns"]
            }
        ),
        types.Tool(
            name="register_code_values",
            description="ì½”ë“œ ì •ì˜ ì •ë³´ ë“±ë¡ (DBë³„)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "code_definitions": {"type": "array", "description": "ì½”ë“œ ì •ì˜ ë°°ì—´"}
                },
                "required": ["database_sid", "code_definitions"]
            }
        ),
        types.Tool(
            name="view_common_metadata",
            description="ë“±ë¡ëœ ê³µí†µ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (DBë³„)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"}
                },
                "required": ["database_sid"]
            }
        ),
        types.Tool(
            name="import_common_columns_csv",
            description="CSV íŒŒì¼ë¡œë¶€í„° ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ì¼ê´„ ë“±ë¡",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "csv_file_path": {"type": "string", "description": "CSV íŒŒì¼ ê²½ë¡œ"}
                },
                "required": ["database_sid", "csv_file_path"]
            }
        ),
        types.Tool(
            name="import_code_definitions_csv",
            description="CSV íŒŒì¼ë¡œë¶€í„° ì½”ë“œ ì •ì˜ ì¼ê´„ ë“±ë¡",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "csv_file_path": {"type": "string", "description": "CSV íŒŒì¼ ê²½ë¡œ"}
                },
                "required": ["database_sid", "csv_file_path"]
            }
        ),
        types.Tool(
            name="import_table_info_csv",
            description="CSV íŒŒì¼ë¡œë¶€í„° í…Œì´ë¸” ì •ë³´ ì¼ê´„ ë“±ë¡",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "csv_file_path": {"type": "string", "description": "CSV íŒŒì¼ ê²½ë¡œ"}
                },
                "required": ["database_sid", "schema_name", "csv_file_path"]
            }
        ),
        types.Tool(
            name="generate_csv_from_schema",
            description="DB ìŠ¤í‚¤ë§ˆì—ì„œ CSV í…œí”Œë¦¿ ìë™ ìƒì„±",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "output_dir": {"type": "string", "description": "ì¶œë ¥ ë””ë ‰í† ë¦¬"}
                },
                "required": ["database_sid", "schema_name", "output_dir"]
            }
        ),
        types.Tool(
            name="extract_and_integrate_metadata",
            description="DB ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ë° ê³µí†µ ë©”íƒ€ë°ì´í„° í†µí•©",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "table_names": {"type": "array", "description": "í…Œì´ë¸” ì´ë¦„ ëª©ë¡ (ì„ íƒ)"},
                    "table_info_csv_path": {"type": "string", "description": "í…Œì´ë¸” ì •ë³´ CSV ê²½ë¡œ (ì„ íƒ)"}
                },
                "required": ["database_sid", "schema_name"]
            }
        ),
        types.Tool(
            name="show_databases",
            description="ë“±ë¡ëœ DB ëª©ë¡ ì¡°íšŒ",
            inputSchema={"type": "object", "properties": {}}
        ),
        types.Tool(
            name="show_connection_status",
            description="ì ‘ì† ê°€ëŠ¥í•œ DB ëª©ë¡ê³¼ ì—°ê²° ì •ë³´ ìƒíƒœ ë³´ê³ ",
            inputSchema={"type": "object", "properties": {}}
        ),
        types.Tool(
            name="show_schemas",
            description="íŠ¹ì • DBì˜ ìŠ¤í‚¤ë§ˆ ëª©ë¡ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"}
                },
                "required": ["database_sid"]
            }
        ),
        types.Tool(
            name="show_tables",
            description="íŠ¹ì • ìŠ¤í‚¤ë§ˆì˜ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "table_filter": {"type": "string", "description": "í…Œì´ë¸” ì´ë¦„ í•„í„° (LIKE íŒ¨í„´, ì˜ˆ: 'ISYS_%', '%_MASTER'). ì„ íƒì‚¬í•­."}
                },
                "required": ["database_sid", "schema_name"]
            }
        ),
        types.Tool(
            name="describe_table",
            description="í…Œì´ë¸” êµ¬ì¡° ìƒì„¸ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "table_name": {"type": "string", "description": "í…Œì´ë¸” ì´ë¦„"}
                },
                "required": ["database_sid", "schema_name", "table_name"]
            }
        ),
        types.Tool(
            name="show_procedures",
            description="íŠ¹ì • ìŠ¤í‚¤ë§ˆì˜ í”„ë¡œì‹œì € ëª©ë¡ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"}
                },
                "required": ["database_sid", "schema_name"]
            }
        ),
        types.Tool(
            name="show_procedure_source",
            description="í”„ë¡œì‹œì € ì†ŒìŠ¤ ì½”ë“œ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "procedure_name": {"type": "string", "description": "í”„ë¡œì‹œì € ì´ë¦„"}
                },
                "required": ["database_sid", "schema_name", "procedure_name"]
            }
        ),
        types.Tool(
            name="execute_sql",
            description="SQL ì¿¼ë¦¬ ì‹¤í–‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "sql": {"type": "string", "description": "ì‹¤í–‰í•  SQL"},
                    "max_rows": {"type": "integer", "description": "ìµœëŒ€ ì¡°íšŒ í–‰ ìˆ˜"}
                },
                "required": ["database_sid", "sql"]
            }
        ),
        types.Tool(
            name="get_table_summaries_for_query",
            description="Stage 1: ìì—°ì–´ ì§ˆì˜ë¥¼ ìœ„í•œ í…Œì´ë¸” ìš”ì•½ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"}
                },
                "required": ["database_sid", "schema_name"]
            }
        ),
        types.Tool(
            name="get_detailed_metadata_for_sql",
            description="Stage 2: SQL ìƒì„±ì„ ìœ„í•œ ìƒì„¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "table_names": {"type": "array", "description": "í…Œì´ë¸” ì´ë¦„ ëª©ë¡"}
                },
                "required": ["database_sid", "schema_name", "table_names"]
            }
        ),
        types.Tool(
            name="get_table_metadata",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ í†µí•© ë©”íƒ€ë°ì´í„° ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_sid": {"type": "string", "description": "Database SID"},
                    "schema_name": {"type": "string", "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„"},
                    "table_name": {"type": "string", "description": "í…Œì´ë¸” ì´ë¦„"}
                },
                "required": ["database_sid", "schema_name", "table_name"]
            }
        ),
    ]


# ============================================
# Tool ì‹¤í–‰ ë¼ìš°í„°
# ============================================
@server.call_tool()
async def handle_call_tool(name: str, arguments: dict):
    """ë‹¨ì¼ Tool ë¼ìš°í„° - ëª¨ë“  Tool í˜¸ì¶œì„ ì ì ˆí•œ í•¨ìˆ˜ë¡œ ë¼ìš°íŒ…"""
    import mcp.types as types

    try:
        # Tool ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ í˜¸ì¶œ
        if name == "register_database_credentials":
            result = await register_database_credentials(**arguments)
        elif name == "delete_database":
            result = await delete_database(**arguments)
        elif name == "load_tnsnames":
            result = await load_tnsnames(**arguments)
        elif name == "list_available_databases":
            result = await list_available_databases(**arguments)
        elif name == "connect_database":
            result = await connect_database(**arguments)
        elif name == "register_common_columns":
            result = await register_common_columns(**arguments)
        elif name == "register_code_values":
            result = await register_code_values(**arguments)
        elif name == "view_common_metadata":
            result = await view_common_metadata(**arguments)
        elif name == "import_common_columns_csv":
            result = await import_common_columns_csv(**arguments)
        elif name == "import_code_definitions_csv":
            result = await import_code_definitions_csv(**arguments)
        elif name == "import_table_info_csv":
            result = await import_table_info_csv(**arguments)
        elif name == "generate_csv_from_schema":
            result = await generate_csv_from_schema(**arguments)
        elif name == "extract_and_integrate_metadata":
            result = await extract_and_integrate_metadata(**arguments)
        elif name == "show_databases":
            result = await show_databases(**arguments)
        elif name == "show_connection_status":
            result = await show_connection_status(**arguments)
        elif name == "show_schemas":
            result = await show_schemas(**arguments)
        elif name == "show_tables":
            result = await show_tables(**arguments)
        elif name == "describe_table":
            result = await describe_table(**arguments)
        elif name == "show_procedures":
            result = await show_procedures(**arguments)
        elif name == "show_procedure_source":
            result = await show_procedure_source(**arguments)
        elif name == "execute_sql":
            result = await execute_sql(**arguments)
        elif name == "get_table_summaries_for_query":
            result = await get_table_summaries_for_query(**arguments)
        elif name == "get_detailed_metadata_for_sql":
            result = await get_detailed_metadata_for_sql(**arguments)
        elif name == "get_table_metadata":
            result = await get_table_metadata(**arguments)
        else:
            return types.CallToolResult(
                content=[types.TextContent(type="text", text=f"âŒ Unknown tool: {name}")],
                isError=True
            )

        # ê²°ê³¼ê°€ ì´ë¯¸ list[dict] í˜•íƒœë¼ë©´ ë³€í™˜
        if isinstance(result, list):
            content = [types.TextContent(type=item.get("type", "text"), text=item.get("text", "")) for item in result]
            return types.CallToolResult(content=content)
        else:
            return types.CallToolResult(
                content=[types.TextContent(type="text", text=str(result))]
            )

    except Exception as e:
        return types.CallToolResult(
            content=[types.TextContent(type="text", text=f"âŒ ì—ëŸ¬: {str(e)}")],
            isError=True
        )


# ============================================
# Tool 1: DB ì ‘ì† ì •ë³´ ë“±ë¡
# ============================================
async def register_database_credentials(
    database_sid: str,
    host: str,
    port: int,
    service_name: str,
    user: str,
    password: str
) -> list[dict]:
    """DB ì ‘ì† ì •ë³´ ì•”í˜¸í™”í•˜ì—¬ ì €ì¥"""
    try:
        credentials = {
            'host': host,
            'port': port,
            'service_name': service_name,
            'user': user,
            'password': password
        }

        success = credentials_manager.save_credentials(database_sid, credentials)

        if success:
            return [{
                "type": "text",
                "text": f"âœ… DB ì ‘ì† ì •ë³´ ì €ì¥ ì™„ë£Œ: {database_sid}"
            }]
        else:
            return [{
                "type": "text",
                "text": f"âŒ DB ì ‘ì† ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {database_sid}"
            }]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 2: DB ì‚­ì œ
# ============================================
async def delete_database(database_sid: str) -> list[dict]:
    """ë“±ë¡ëœ DB ì‚­ì œ (ì ‘ì† ì •ë³´ ë° ëª¨ë“  ë©”íƒ€ë°ì´í„° ì‚­ì œ)"""
    try:
        from pathlib import Path
        import shutil

        # 1. DBê°€ ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        databases = credentials_manager.list_databases()
        if database_sid not in databases:
            return [{
                "type": "text",
                "text": f"âŒ ë“±ë¡ë˜ì§€ ì•Šì€ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤: {database_sid}"
            }]

        result_text = f"ğŸ—‘ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ: {database_sid}\n\n"
        deleted_items = []

        # 2. ì»¤ë„¥í„° ìºì‹œì—ì„œ ì œê±°
        if database_sid in db_connectors:
            try:
                db_connectors[database_sid].disconnect()
            except:
                pass
            del db_connectors[database_sid]
            deleted_items.append("âœ… í™œì„± ì—°ê²° í•´ì œ")

        # 3. ì ‘ì† ì •ë³´ ì‚­ì œ
        if credentials_manager.delete_credentials(database_sid):
            deleted_items.append("âœ… ì ‘ì† ì •ë³´ ì‚­ì œ")
        else:
            deleted_items.append("âš ï¸ ì ‘ì† ì •ë³´ ì‚­ì œ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ")

        # 4. ê³µí†µ ë©”íƒ€ë°ì´í„° ì‚­ì œ
        common_metadata_dir = Path("./common_metadata") / database_sid
        if common_metadata_dir.exists():
            shutil.rmtree(common_metadata_dir)
            deleted_items.append("âœ… ê³µí†µ ë©”íƒ€ë°ì´í„° ì‚­ì œ")
        else:
            deleted_items.append("âš ï¸ ê³µí†µ ë©”íƒ€ë°ì´í„° ì—†ìŒ")

        # 5. í†µí•© ë©”íƒ€ë°ì´í„° ì‚­ì œ
        metadata_dir = Path("./metadata") / database_sid
        if metadata_dir.exists():
            shutil.rmtree(metadata_dir)
            deleted_items.append("âœ… í†µí•© ë©”íƒ€ë°ì´í„° ì‚­ì œ")
        else:
            deleted_items.append("âš ï¸ í†µí•© ë©”íƒ€ë°ì´í„° ì—†ìŒ")

        result_text += "**ì‚­ì œ í•­ëª©**:\n"
        for item in deleted_items:
            result_text += f"- {item}\n"

        result_text += f"\nâœ… ë°ì´í„°ë² ì´ìŠ¤ '{database_sid}' ì‚­ì œ ì™„ë£Œ"

        return [{
            "type": "text",
            "text": result_text
        }]

    except Exception as e:
        logger.error(f"DB ì‚­ì œ ì‹¤íŒ¨: {e}")
        import traceback
        return [{
            "type": "text",
            "text": f"âŒ DB ì‚­ì œ ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
        }]


# ============================================
# Tool 3: tnsnames.ora íŒŒì¼ ë¡œë“œ
# ============================================

async def load_tnsnames(
    tnsnames_path: str
) -> list[dict]:
    """
    tnsnames.ora íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ DB ëª©ë¡ ì¶”ì¶œ ë° ìºì‹±

    Args:
        tnsnames_path: tnsnames.ora íŒŒì¼ ì „ì²´ ê²½ë¡œ
    """
    try:
        import json

        # tnsnames.ora íŒŒì‹±
        databases = tnsnames_parser.parse_file(tnsnames_path)

        # ìºì‹œ íŒŒì¼ì— ì €ì¥
        with open(TNSNAMES_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(databases, f, ensure_ascii=False, indent=2)

        result_text = f"âœ… tnsnames.ora íŒŒì‹± ì™„ë£Œ\n\n"
        result_text += f"**íŒŒì¼ ê²½ë¡œ**: {tnsnames_path}\n"
        result_text += f"**ë°œê²¬ëœ DB ìˆ˜**: {len(databases)}ê°œ\n"
        result_text += f"**ìºì‹œ ì €ì¥ ìœ„ì¹˜**: {TNSNAMES_CACHE_FILE}\n\n"
        result_text += "**ë‹¤ìŒ ë‹¨ê³„**: `list_available_databases` Toolì„ í˜¸ì¶œí•˜ì—¬ DB ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”."

        return [{
            "type": "text",
            "text": result_text
        }]

    except FileNotFoundError:
        return [{
            "type": "text",
            "text": f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tnsnames_path}"
        }]
    except Exception as e:
        logger.error(f"tnsnames ë¡œë“œ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ tnsnames.ora íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 3: ì‚¬ìš© ê°€ëŠ¥í•œ DB ëª©ë¡ ì¡°íšŒ
# ============================================

async def list_available_databases(
    keyword: str = ""
) -> list[dict]:
    """
    tnsnames.oraì—ì„œ ì¶”ì¶œí•œ DB ëª©ë¡ ì¡°íšŒ

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì„ íƒ, DBëª… ë˜ëŠ” ì„¤ëª…ì—ì„œ ê²€ìƒ‰)
    """
    try:
        import json

        # ìºì‹œ íŒŒì¼ í™•ì¸
        if not TNSNAMES_CACHE_FILE.exists():
            return [{
                "type": "text",
                "text": "âŒ tnsnames ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € `load_tnsnames` Toolì„ í˜¸ì¶œí•˜ì—¬ tnsnames.ora íŒŒì¼ì„ ë¡œë“œí•˜ì„¸ìš”."
            }]

        # ìºì‹œ íŒŒì¼ ì½ê¸°
        with open(TNSNAMES_CACHE_FILE, 'r', encoding='utf-8') as f:
            databases = json.load(f)

        # í‚¤ì›Œë“œ í•„í„°ë§
        if keyword:
            keyword_lower = keyword.lower()
            filtered = {
                db_sid: info for db_sid, info in databases.items()
                if keyword_lower in db_sid.lower() or
                   keyword_lower in info.get('description', '').lower()
            }
        else:
            filtered = databases

        result_text = f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡\n\n"

        if keyword:
            result_text += f"**ê²€ìƒ‰ í‚¤ì›Œë“œ**: {keyword}\n"
            result_text += f"**ê²€ìƒ‰ ê²°ê³¼**: {len(filtered)}ê°œ\n\n"
        else:
            result_text += f"**ì „ì²´ DB ìˆ˜**: {len(filtered)}ê°œ\n\n"

        # DB ëª©ë¡ (ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ)
        count = 0
        for db_sid in sorted(filtered.keys()):
            if count >= 20:
                result_text += f"\n... ì™¸ {len(filtered) - 20}ê°œ ë” ìˆìŒ\n"
                break

            info = filtered[db_sid]
            result_text += f"### {db_sid}\n"
            if info.get('description'):
                result_text += f"  - **ì„¤ëª…**: {info['description']}\n"
            result_text += f"  - **í˜¸ìŠ¤íŠ¸**: {info['host']}:{info['port']}\n"
            result_text += f"  - **ì„œë¹„ìŠ¤ëª…**: {info['service_name']}\n"
            result_text += f"  - **ì—°ê²°ë°©ì‹**: {info['connection_type']}\n\n"
            count += 1

        result_text += "\n**ë‹¤ìŒ ë‹¨ê³„**: ì‚¬ìš©í•  DBë¥¼ ì„ íƒí•˜ê³  `connect_database` Toolë¡œ ì—°ê²°í•˜ì„¸ìš”."

        return [{
            "type": "text",
            "text": result_text
        }]

    except Exception as e:
        logger.error(f"DB ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ DB ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 4: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì €ì¥
# ============================================

async def connect_database(
    database_sid: str,
    user: str,
    password: str
) -> list[dict]:
    """
    tnsnamesì—ì„œ ì¶”ì¶œí•œ DBì— ì—°ê²° ë° ì ‘ì†ì •ë³´ ì €ì¥

    Args:
        database_sid: tnsnamesì˜ DB SID (ì˜ˆ: SOLUM, JSTECH)
        user: Oracle ì‚¬ìš©ìëª… (ì˜ˆ: scott, system)
        password: Oracle ë¹„ë°€ë²ˆí˜¸
    """
    try:
        import json

        # ìºì‹œì—ì„œ DB ì •ë³´ ì¡°íšŒ
        if not TNSNAMES_CACHE_FILE.exists():
            return [{
                "type": "text",
                "text": "âŒ tnsnames ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.\në¨¼ì € `load_tnsnames` Toolì„ í˜¸ì¶œí•˜ì„¸ìš”."
            }]

        with open(TNSNAMES_CACHE_FILE, 'r', encoding='utf-8') as f:
            databases = json.load(f)

        if database_sid not in databases:
            return [{
                "type": "text",
                "text": f"âŒ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {database_sid}\n`list_available_databases` Toolë¡œ DB ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”."
            }]

        db_info = databases[database_sid]

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        connector = OracleConnector(
            host=db_info['host'],
            port=db_info['port'],
            service_name=db_info['service_name'],
            user=user,
            password=password
        )

        connector.connect()

        # ì—°ê²° ì„±ê³µ ì‹œ credentials ì €ì¥
        credentials = {
            'host': db_info['host'],
            'port': db_info['port'],
            'service_name': db_info['service_name'],
            'user': user,
            'password': password
        }

        success = credentials_manager.save_credentials(database_sid, credentials)

        if success:
            result_text = f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ ë° ì €ì¥ ì™„ë£Œ\n\n"
            result_text += f"**Database SID**: {database_sid}\n"
            result_text += f"**í˜¸ìŠ¤íŠ¸**: {db_info['host']}:{db_info['port']}\n"
            result_text += f"**ì„œë¹„ìŠ¤ëª…**: {db_info['service_name']}\n"
            result_text += f"**ì‚¬ìš©ì**: {user}\n"
            if db_info.get('description'):
                result_text += f"**ì„¤ëª…**: {db_info['description']}\n"
            result_text += f"\nâœ… ì ‘ì† ì •ë³´ê°€ ì•”í˜¸í™”ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            result_text += f"ì´ì œ ì´ DBë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
            result_text += "**ë‹¤ìŒ ë‹¨ê³„**: \n"
            result_text += f"- `show_schemas` Toolë¡œ ìŠ¤í‚¤ë§ˆ ëª©ë¡ í™•ì¸\n"
            result_text += f"- `extract_and_integrate_metadata` Toolë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"

            # ìºì‹œì—ì„œ ì»¤ë„¥í„° ì œê±° (ìƒˆë¡œ ì—°ê²°í•˜ë„ë¡)
            if database_sid in db_connectors:
                del db_connectors[database_sid]

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": f"âŒ ì ‘ì† ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {database_sid}"
            }]

    except Exception as e:
        logger.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}\n\nì‚¬ìš©ìëª…ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        }]


# ============================================
# Tool 5: ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ë“±ë¡
# ============================================

async def register_common_columns(
    database_sid: str,
    columns: list
) -> list[dict]:
    """
    ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ë“±ë¡ (DBë³„)

    Args:
        database_sid: Database SID
        columns: ì¹¼ëŸ¼ ì •ë³´ (JSON í˜•ì‹ ë¬¸ìì—´)
            ì˜ˆ: '[{"column_name":"STATUS","korean_name":"ìƒíƒœ","description":"ì²˜ë¦¬ ìƒíƒœ","is_code_column":true,...}]'
    """
    try:
        import json

        # JSON íŒŒì‹±
        columns = json.loads(columns)

        # ì €ì¥
        success = common_metadata_manager.save_common_columns(database_sid, columns)

        if success:
            stats = common_metadata_manager.get_statistics(database_sid)

            result_text = f"âœ… ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ë“±ë¡ ì™„ë£Œ\n\n"
            result_text += f"**ë“±ë¡ëœ ì¹¼ëŸ¼ ìˆ˜**: {len(columns)}ê°œ\n"
            result_text += f"**ì „ì²´ ì¹¼ëŸ¼ ìˆ˜**: {stats['common_column_count']}ê°œ\n\n"
            result_text += "**ë“±ë¡ëœ ì¹¼ëŸ¼**:\n"
            for col in columns:
                result_text += f"- {col['column_name']}: {col.get('korean_name', 'N/A')}"
                if col.get('is_code_column'):
                    result_text += " (ì½”ë“œì¹¼ëŸ¼)"
                result_text += "\n"

            result_text += "\n**ë‹¤ìŒ ë‹¨ê³„**:\n"
            result_text += "- ì½”ë“œ ì¹¼ëŸ¼ì¸ ê²½ìš° `register_code_values` Toolë¡œ ì½”ë“œ ì •ë³´ ë“±ë¡\n"
            result_text += "- `generate_csv_from_schema` Toolë¡œ CSV íŒŒì¼ ìë™ ìƒì„±"

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": "âŒ ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ì €ì¥ ì‹¤íŒ¨"
            }]

    except json.JSONDecodeError as e:
        return [{
            "type": "text",
            "text": f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì˜¬ë°”ë¥¸ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
        }]
    except Exception as e:
        logger.error(f"ê³µí†µ ì¹¼ëŸ¼ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ë“±ë¡ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 6: ì½”ë“œ ì •ë³´ ë“±ë¡
# ============================================

async def register_code_values(
    database_sid: str,
    code_definitions: list
) -> list[dict]:
    """
    ì½”ë“œ ì •ë³´ ë“±ë¡ (DBë³„)

    Args:
        database_sid: Database SID
        code_definitions: ì½”ë“œ ì •ë³´ (JSON í˜•ì‹ ë¬¸ìì—´)
            ì˜ˆ: '[{"column_name":"STATUS","code_value":"01","code_label":"ì ‘ìˆ˜","code_description":"ì ‘ìˆ˜ë¨",...}]'
    """
    try:
        import json

        # JSON íŒŒì‹±
        codes = json.loads(code_definitions)

        # ì €ì¥
        success = common_metadata_manager.save_code_definitions(database_sid, codes)

        if success:
            stats = common_metadata_manager.get_statistics(database_sid)

            # ì¹¼ëŸ¼ë³„ ê·¸ë£¹í™”
            by_column = {}
            for code in codes:
                col = code['column_name']
                if col not in by_column:
                    by_column[col] = []
                by_column[col].append(code)

            result_text = f"âœ… ì½”ë“œ ì •ë³´ ë“±ë¡ ì™„ë£Œ\n\n"
            result_text += f"**ë“±ë¡ëœ ì½”ë“œ ìˆ˜**: {len(codes)}ê°œ\n"
            result_text += f"**ì½”ë“œ ì¹¼ëŸ¼ ìˆ˜**: {len(by_column)}ê°œ\n"
            result_text += f"**ì „ì²´ ì½”ë“œ ìˆ˜**: {stats['total_code_count']}ê°œ\n\n"
            result_text += "**ë“±ë¡ëœ ì½”ë“œ**:\n"
            for column_name, column_codes in by_column.items():
                result_text += f"\n### {column_name}\n"
                for code in sorted(column_codes, key=lambda x: x.get('display_order', 999)):
                    result_text += f"  - {code['code_value']}: {code.get('code_label', 'N/A')}\n"

            result_text += "\n**ë‹¤ìŒ ë‹¨ê³„**:\n"
            result_text += "`generate_csv_from_schema` Toolë¡œ CSV íŒŒì¼ ìë™ ìƒì„±"

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": "âŒ ì½”ë“œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨"
            }]

    except json.JSONDecodeError as e:
        return [{
            "type": "text",
            "text": f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì˜¬ë°”ë¥¸ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
        }]
    except Exception as e:
        logger.error(f"ì½”ë“œ ì •ë³´ ë“±ë¡ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ì½”ë“œ ì •ë³´ ë“±ë¡ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 7: ê³µí†µ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
# ============================================

async def view_common_metadata(
    database_sid: str,
    metadata_type: str = "all"
) -> list[dict]:
    """
    ì €ì¥ëœ ê³µí†µ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (DBë³„)

    Args:
        database_sid: Database SID
        metadata_type: ì¡°íšŒ ìœ í˜• ("all", "columns", "codes", "stats")
    """
    try:
        import json

        result_text = f"ğŸ“Š ê³µí†µ ë©”íƒ€ë°ì´í„° ì¡°íšŒ - {database_sid}\n\n"

        if metadata_type in ["all", "stats"]:
            stats = common_metadata_manager.get_statistics(database_sid)
            result_text += "## í†µê³„\n"
            result_text += f"- **ê³µí†µ ì¹¼ëŸ¼ ìˆ˜**: {stats['common_column_count']}ê°œ\n"
            result_text += f"- **ì½”ë“œ ì¹¼ëŸ¼ ìˆ˜**: {stats['code_column_count']}ê°œ\n"
            result_text += f"- **ì „ì²´ ì½”ë“œ ìˆ˜**: {stats['total_code_count']}ê°œ\n\n"

        if metadata_type in ["all", "columns"]:
            columns = common_metadata_manager.load_common_columns(database_sid)
            result_text += "## ê³µí†µ ì¹¼ëŸ¼ ì •ë³´\n\n"
            for column_name in sorted(columns.keys()):
                col = columns[column_name]
                result_text += f"### {column_name}\n"
                result_text += f"- **í•œê¸€ëª…**: {col.get('korean_name', 'N/A')}\n"
                result_text += f"- **ì„¤ëª…**: {col.get('description', 'N/A')}\n"
                result_text += f"- **ì½”ë“œì¹¼ëŸ¼**: {'ì˜ˆ' if col.get('is_code_column') else 'ì•„ë‹ˆì˜¤'}\n"
                if col.get('business_rule'):
                    result_text += f"- **ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™**: {col['business_rule']}\n"
                result_text += "\n"

        if metadata_type in ["all", "codes"]:
            code_defs = common_metadata_manager.load_code_definitions(database_sid)
            result_text += "## ì½”ë“œ ì •ë³´\n\n"
            for column_name in sorted(code_defs.keys()):
                codes = code_defs[column_name]
                result_text += f"### {column_name}\n"
                for code_value in sorted(codes.keys()):
                    code = codes[code_value]
                    result_text += f"  - **{code_value}**: {code.get('code_label', 'N/A')} - {code.get('code_description', '')}\n"
                result_text += "\n"

        return [{
            "type": "text",
            "text": result_text
        }]

    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 8: ê³µí†µ ì¹¼ëŸ¼ CSV ì¼ê´„ ë“±ë¡
# ============================================

async def import_common_columns_csv(
    database_sid: str,
    csv_file_path: str
) -> list[dict]:
    """
    CSV íŒŒì¼ë¡œë¶€í„° ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ì¼ê´„ ë“±ë¡

    Args:
        database_sid: Database SID
        csv_file_path: ê³µí†µ ì¹¼ëŸ¼ CSV íŒŒì¼ ê²½ë¡œ
            í˜•ì‹: column_name,korean_name,description,is_code_column,sample_values,business_rule,unit,aggregation_functions,is_sensitive
    """
    try:
        import csv
        from pathlib import Path

        csv_path = Path(csv_file_path)
        if not csv_path.exists():
            return [{
                "type": "text",
                "text": f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}"
            }]

        columns = []
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                column = {
                    'column_name': row['column_name'],
                    'korean_name': row.get('korean_name', ''),
                    'description': row.get('description', ''),
                    'is_code_column': row.get('is_code_column', 'N').upper() == 'Y',
                    'sample_values': row.get('sample_values', ''),
                    'business_rule': row.get('business_rule', ''),
                    'unit': row.get('unit', ''),
                    'aggregation_functions': row.get('aggregation_functions', ''),
                    'is_sensitive': row.get('is_sensitive', 'N').upper() == 'Y'
                }
                columns.append(column)

        # ì €ì¥
        success = common_metadata_manager.save_common_columns(database_sid, columns)

        if success:
            stats = common_metadata_manager.get_statistics(database_sid)

            result_text = f"âœ… ê³µí†µ ì¹¼ëŸ¼ CSV ì¼ê´„ ë“±ë¡ ì™„ë£Œ\n\n"
            result_text += f"**Database**: {database_sid}\n"
            result_text += f"**CSV íŒŒì¼**: {csv_file_path}\n"
            result_text += f"**ë“±ë¡ëœ ì¹¼ëŸ¼ ìˆ˜**: {len(columns)}ê°œ\n"
            result_text += f"**ì „ì²´ ì¹¼ëŸ¼ ìˆ˜**: {stats['common_column_count']}ê°œ\n\n"
            result_text += "**ë“±ë¡ëœ ì¹¼ëŸ¼ ëª©ë¡**:\n"
            for col in columns[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                result_text += f"- {col['column_name']}: {col['korean_name']}\n"
            if len(columns) > 10:
                result_text += f"- ... ì™¸ {len(columns) - 10}ê°œ\n"

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": "âŒ ê³µí†µ ì¹¼ëŸ¼ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨"
            }]

    except Exception as e:
        logger.error(f"CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {e}")
        import traceback
        return [{
            "type": "text",
            "text": f"âŒ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
        }]


# ============================================
# Tool 9: ì½”ë“œ ì •ì˜ CSV ì¼ê´„ ë“±ë¡
# ============================================

async def import_code_definitions_csv(
    database_sid: str,
    csv_file_path: str
) -> list[dict]:
    """
    CSV íŒŒì¼ë¡œë¶€í„° ì½”ë“œ ì •ì˜ ì¼ê´„ ë“±ë¡

    Args:
        database_sid: Database SID
        csv_file_path: ì½”ë“œ ì •ì˜ CSV íŒŒì¼ ê²½ë¡œ
            í˜•ì‹: column_name,code_value,code_label,code_description,display_order,is_active,parent_code,state_transition
    """
    try:
        import csv
        from pathlib import Path

        csv_path = Path(csv_file_path)
        if not csv_path.exists():
            return [{
                "type": "text",
                "text": f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}"
            }]

        codes = []
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                code = {
                    'column_name': row['column_name'],
                    'code_value': row['code_value'],
                    'code_label': row.get('code_label', ''),
                    'code_description': row.get('code_description', ''),
                    'display_order': int(row.get('display_order', 999)) if row.get('display_order') else 999,
                    'is_active': row.get('is_active', 'Y').upper() == 'Y',
                    'parent_code': row.get('parent_code', ''),
                    'state_transition': row.get('state_transition', '')
                }
                codes.append(code)

        # ì €ì¥
        success = common_metadata_manager.save_code_definitions(database_sid, codes)

        if success:
            stats = common_metadata_manager.get_statistics(database_sid)

            # ì¹¼ëŸ¼ë³„ ê·¸ë£¹í™”
            by_column = {}
            for code in codes:
                col_name = code['column_name']
                if col_name not in by_column:
                    by_column[col_name] = 0
                by_column[col_name] += 1

            result_text = f"âœ… ì½”ë“œ ì •ì˜ CSV ì¼ê´„ ë“±ë¡ ì™„ë£Œ\n\n"
            result_text += f"**Database**: {database_sid}\n"
            result_text += f"**CSV íŒŒì¼**: {csv_file_path}\n"
            result_text += f"**ë“±ë¡ëœ ì½”ë“œ ìˆ˜**: {len(codes)}ê°œ\n"
            result_text += f"**ì½”ë“œ ì¹¼ëŸ¼ ìˆ˜**: {len(by_column)}ê°œ\n\n"
            result_text += "**ì¹¼ëŸ¼ë³„ ì½”ë“œ ìˆ˜**:\n"
            for col_name, count in sorted(by_column.items()):
                result_text += f"- {col_name}: {count}ê°œ\n"

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": "âŒ ì½”ë“œ ì •ì˜ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨"
            }]

    except Exception as e:
        logger.error(f"CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {e}")
        import traceback
        return [{
            "type": "text",
            "text": f"âŒ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
        }]


# ============================================
# Tool 10: í…Œì´ë¸” ì •ë³´ CSV ì¼ê´„ ë“±ë¡
# ============================================

async def import_table_info_csv(
    database_sid: str,
    schema_name: str,
    csv_file_path: str
) -> list[dict]:
    """
    CSV íŒŒì¼ë¡œë¶€í„° í…Œì´ë¸” ì •ë³´ ì¼ê´„ ë“±ë¡

    Args:
        database_sid: Database SID
        schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„
        csv_file_path: í…Œì´ë¸” ì •ë³´ CSV íŒŒì¼ ê²½ë¡œ
            í˜•ì‹: table_name,business_purpose,usage_scenario_1,usage_scenario_2,usage_scenario_3,related_tables
    """
    try:
        import csv
        from pathlib import Path

        csv_path = Path(csv_file_path)
        if not csv_path.exists():
            return [{
                "type": "text",
                "text": f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}"
            }]

        tables_info = []
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # usage_scenarios ë¦¬ìŠ¤íŠ¸ ìƒì„±
                usage_scenarios = []
                for i in range(1, 4):
                    scenario = row.get(f'usage_scenario_{i}', '').strip()
                    if scenario:
                        usage_scenarios.append(scenario)

                # related_tables ë¦¬ìŠ¤íŠ¸ ìƒì„±
                related_tables_str = row.get('related_tables', '').strip()
                related_tables = []
                if related_tables_str:
                    related_tables = [t.strip() for t in related_tables_str.split(',') if t.strip()]

                table_info = {
                    'table_name': row['table_name'],
                    'business_purpose': row.get('business_purpose', ''),
                    'usage_scenarios': usage_scenarios,
                    'related_tables': related_tables
                }
                tables_info.append(table_info)

        # ì €ì¥
        success = common_metadata_manager.save_table_info(database_sid, schema_name, tables_info)

        if success:
            result_text = f"âœ… í…Œì´ë¸” ì •ë³´ CSV ì¼ê´„ ë“±ë¡ ì™„ë£Œ\n\n"
            result_text += f"**Database**: {database_sid}\n"
            result_text += f"**Schema**: {schema_name}\n"
            result_text += f"**CSV íŒŒì¼**: {csv_file_path}\n"
            result_text += f"**ë“±ë¡ëœ í…Œì´ë¸” ìˆ˜**: {len(tables_info)}ê°œ\n\n"
            result_text += "**ë“±ë¡ëœ í…Œì´ë¸” ëª©ë¡**:\n"
            for table in tables_info[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                result_text += f"- {table['table_name']}: {table['business_purpose']}\n"
            if len(tables_info) > 10:
                result_text += f"- ... ì™¸ {len(tables_info) - 10}ê°œ\n"

            return [{
                "type": "text",
                "text": result_text
            }]
        else:
            return [{
                "type": "text",
                "text": "âŒ í…Œì´ë¸” ì •ë³´ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨"
            }]

    except Exception as e:
        logger.error(f"CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {e}")
        import traceback
        return [{
            "type": "text",
            "text": f"âŒ CSV ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
        }]


# ============================================
# Tool 11: CSV íŒŒì¼ ìë™ ìƒì„±
# ============================================

async def generate_csv_from_schema(
    database_sid: str,
    schema_name: str
) -> list[dict]:
    """
    DB ìŠ¤í‚¤ë§ˆ ì •ë³´ + ê³µí†µ ë©”íƒ€ë°ì´í„° â†’ CSV íŒŒì¼ ìë™ ìƒì„±

    Args:
        database_sid: Database SID
        schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„
    """
    try:
        # DB ì—°ê²°
        connector = get_connector(database_sid)

        # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        tables = connector.list_tables(schema_name)

        if not tables:
            return [{
                "type": "text",
                "text": f"âŒ {schema_name} ìŠ¤í‚¤ë§ˆì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤."
            }]

        # ê° í…Œì´ë¸”ì˜ ì¹¼ëŸ¼ ì •ë³´ ì¶”ì¶œ
        tables_columns = {}
        for table in tables:
            table_name = table['table_name']
            columns = connector.extract_table_columns(schema_name, table_name)
            tables_columns[table_name] = columns

        # CSV íŒŒì¼ ìƒì„±
        result = common_metadata_manager.generate_csv_files(
            database_sid,
            schema_name,
            tables_columns
        )

        result_text = f"âœ… CSV íŒŒì¼ ìë™ ìƒì„± ì™„ë£Œ\n\n"
        result_text += f"**Database**: {database_sid}\n"
        result_text += f"**Schema**: {schema_name}\n"
        result_text += f"**í…Œì´ë¸” ìˆ˜**: {len(tables_columns)}ê°œ\n\n"
        result_text += "**ìƒì„±ëœ íŒŒì¼**:\n"
        result_text += f"1. `table_info.csv`: {result['table_info']}\n"
        result_text += f"   - ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì  ë“±ì€ ì§ì ‘ ì…ë ¥ í•„ìš” âš ï¸\n\n"
        result_text += f"2. `column_info.csv`: {result['column_info']}\n"
        result_text += f"   - ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ ìë™ ë§¤ì¹­ë¨ âœ…\n\n"
        result_text += f"3. `code_values.csv`: {result['code_values']}\n"
        result_text += f"   - ë“±ë¡ëœ ì½”ë“œ ì •ë³´ ìë™ ë§¤ì¹­ë¨ âœ…\n\n"
        result_text += "**ë‹¤ìŒ ë‹¨ê³„**:\n"
        result_text += "1. `table_info.csv`ë¥¼ ì—´ì–´ ê° í…Œì´ë¸”ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì  ì…ë ¥\n"
        result_text += "2. `column_info.csv` í™•ì¸ ë° ëˆ„ë½ëœ ì •ë³´ ë³´ì™„\n"
        result_text += "3. `extract_and_integrate_metadata` Toolë¡œ ë©”íƒ€ë°ì´í„° í†µí•©"

        return [{
            "type": "text",
            "text": result_text
        }]

    except Exception as e:
        logger.error(f"CSV ìƒì„± ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 9: ë©”íƒ€ì •ë³´ ì¶”ì¶œ ë° í†µí•©
# ============================================

async def extract_and_integrate_metadata(
    database_sid: str,
    schema_name: str,
    table_info_csv_path: str = None
) -> list[dict]:
    """
    DB ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ + ê³µí†µ ë©”íƒ€ë°ì´í„° í†µí•©

    ê³µí†µ ì¹¼ëŸ¼ ì •ë³´ì™€ ì½”ë“œ ì •ë³´ëŠ” register_common_columns/register_code_valuesë¡œ ë¯¸ë¦¬ ë“±ë¡ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    í…Œì´ë¸”ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´(ëª©ì , ì‹œë‚˜ë¦¬ì˜¤)ëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤.

    Args:
        database_sid: Database SID
        schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„
        table_info_csv_path: í…Œì´ë¸” ì •ë³´ CSV ê²½ë¡œ (ì„ íƒì‚¬í•­)
    """
    try:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š ë©”íƒ€ì •ë³´ ì¶”ì¶œ ì‹œì‘: {database_sid}.{schema_name}")
        logger.info(f"{'='*60}\n")

        # DB ì—°ê²°
        connector = get_connector(database_sid)

        # í…Œì´ë¸” ì •ë³´ ë¡œë“œ
        # 1ìˆœìœ„: CSV íŒŒì¼ (íŒŒë¼ë¯¸í„°ë¡œ ì œê³µëœ ê²½ìš°)
        # 2ìˆœìœ„: ì €ì¥ëœ í…Œì´ë¸” ì •ë³´ (import_table_info_csvë¡œ ë“±ë¡ëœ ê²½ìš°)
        table_info_dict = {}

        if table_info_csv_path:
            # CSV íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
            import csv
            with open(table_info_csv_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    table_info_dict[row['table_name']] = {
                        'business_purpose': row.get('business_purpose', ''),
                        'usage_scenarios': [
                            row.get('usage_scenario_1', ''),
                            row.get('usage_scenario_2', ''),
                            row.get('usage_scenario_3', '')
                        ],
                        'related_tables': row.get('related_tables', '').split(',') if row.get('related_tables') else []
                    }
        else:
            # ì €ì¥ëœ í…Œì´ë¸” ì •ë³´ ë¡œë“œ
            table_info_dict = common_metadata_manager.load_table_info(database_sid, schema_name)

        # í…Œì´ë¸” ëª©ë¡
        tables = connector.list_tables(schema_name)

        processed_tables = []

        for table_info in tables:
            table_name = table_info['TABLE_NAME']

            logger.info(f"ì²˜ë¦¬ ì¤‘: {table_name}")

            # DB ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
            db_schema = {
                'columns': connector.extract_table_columns(schema_name, table_name),
                'primary_keys': connector.extract_primary_keys(schema_name, table_name),
                'foreign_keys': connector.extract_foreign_keys(schema_name, table_name),
                'indexes': connector.extract_indexes(schema_name, table_name),
                'table_comment': connector.get_table_comment(schema_name, table_name)
            }

            # ë©”íƒ€ì •ë³´ í†µí•© (ê³µí†µ ë©”íƒ€ë°ì´í„° ìë™ ë§¤ì¹­)
            unified_metadata = metadata_manager.integrate_metadata(
                database_sid,
                schema_name,
                table_name,
                db_schema,
                table_info=table_info_dict.get(table_name)
            )

            # ì €ì¥
            metadata_manager.save_unified_metadata(
                database_sid, schema_name, table_name, unified_metadata
            )

            processed_tables.append(table_name)

        # í…Œì´ë¸” ìš”ì•½ ìƒì„± (Stage 1ìš©)
        metadata_manager.generate_table_summaries(database_sid, schema_name)

        # í†µê³„
        stats = common_metadata_manager.get_statistics(database_sid)

        result_text = f"âœ… ë©”íƒ€ì •ë³´ ì¶”ì¶œ ì™„ë£Œ\n\n"
        result_text += f"**ë°ì´í„°ë² ì´ìŠ¤**: {database_sid}\n"
        result_text += f"**ìŠ¤í‚¤ë§ˆ**: {schema_name}\n"
        result_text += f"**ì²˜ë¦¬ëœ í…Œì´ë¸”**: {len(processed_tables)}ê°œ\n\n"
        result_text += f"**ê³µí†µ ë©”íƒ€ë°ì´í„° í†µê³„**:\n"
        result_text += f"- ê³µí†µ ì¹¼ëŸ¼: {stats['common_column_count']}ê°œ\n"
        result_text += f"- ì½”ë“œ ì¹¼ëŸ¼: {stats['code_column_count']}ê°œ\n"
        result_text += f"- ì „ì²´ ì½”ë“œ ê°’: {stats['total_code_count']}ê°œ\n\n"
        result_text += "**í…Œì´ë¸” ëª©ë¡**:\n"
        result_text += "\n".join([f"- {t}" for t in processed_tables])

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        logger.error(f"ë©”íƒ€ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ë©”íƒ€ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 3: ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡
# ============================================

async def show_databases() -> list[dict]:
    """ë“±ë¡ëœ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡"""
    try:
        databases = credentials_manager.list_databases()

        if not databases:
            return [{
                "type": "text",
                "text": "ë“±ë¡ëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            }]

        result_text = f"ğŸ“‚ ë“±ë¡ëœ ë°ì´í„°ë² ì´ìŠ¤ ({len(databases)}ê°œ)\n\n"
        for db_sid in databases:
            result_text += f"- {db_sid}\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool: ì—°ê²° ìƒíƒœ ë³´ê³ 
# ============================================

async def show_connection_status() -> list[dict]:
    """ì ‘ì† ê°€ëŠ¥í•œ DB ëª©ë¡ê³¼ ì—°ê²° ì •ë³´, ë©”íƒ€ë°ì´í„° ìƒíƒœ ë³´ê³ """
    try:
        import json
        from pathlib import Path

        databases = credentials_manager.list_databases()

        if not databases:
            return [{
                "type": "text",
                "text": "ë“±ë¡ëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            }]

        result_text = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ ë³´ê³ **\n\n"
        result_text += f"ë“±ë¡ëœ ë°ì´í„°ë² ì´ìŠ¤: **{len(databases)}ê°œ**\n\n"
        result_text += "=" * 60 + "\n\n"

        for db_sid in databases:
            result_text += f"## ğŸ—„ï¸ {db_sid}\n\n"

            try:
                # 1. ì—°ê²° ì •ë³´ ë¡œë“œ
                credentials = credentials_manager.load_credentials(db_sid)
                result_text += f"### ğŸ“¡ ì—°ê²° ì •ë³´\n"
                result_text += f"- **í˜¸ìŠ¤íŠ¸**: {credentials['host']}:{credentials['port']}\n"
                result_text += f"- **ì„œë¹„ìŠ¤ëª…**: {credentials['service_name']}\n"
                result_text += f"- **ì‚¬ìš©ì**: {credentials['user']}\n"
                result_text += f"- **ë¹„ë°€ë²ˆí˜¸**: {'*' * len(credentials['password'])}\n\n"

                # 2. ì—°ê²° í…ŒìŠ¤íŠ¸
                try:
                    connector = OracleConnector(
                        host=credentials['host'],
                        port=credentials['port'],
                        service_name=credentials['service_name'],
                        user=credentials['user'],
                        password=credentials['password']
                    )
                    if connector.connect():
                        result_text += f"- **ì—°ê²° ìƒíƒœ**: âœ… ì—°ê²° ê°€ëŠ¥\n\n"

                        # 3. ìŠ¤í‚¤ë§ˆ ëª©ë¡ ì¡°íšŒ
                        try:
                            schemas = connector.list_schemas()
                            result_text += f"### ğŸ“‚ ìŠ¤í‚¤ë§ˆ ëª©ë¡\n"
                            result_text += f"- **ìŠ¤í‚¤ë§ˆ ìˆ˜**: {len(schemas)}ê°œ\n"
                            result_text += f"- **ëª©ë¡**: {', '.join(schemas[:5])}"
                            if len(schemas) > 5:
                                result_text += f" ì™¸ {len(schemas) - 5}ê°œ"
                            result_text += "\n\n"
                        except Exception as e:
                            result_text += f"### ğŸ“‚ ìŠ¤í‚¤ë§ˆ ëª©ë¡\n"
                            result_text += f"- âš ï¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n\n"

                        connector.disconnect()
                    else:
                        result_text += f"- **ì—°ê²° ìƒíƒœ**: âŒ ì—°ê²° ì‹¤íŒ¨\n\n"
                except Exception as e:
                    result_text += f"- **ì—°ê²° ìƒíƒœ**: âŒ ì—°ê²° ì‹¤íŒ¨ ({str(e)})\n\n"

                # 4. ê³µí†µ ë©”íƒ€ë°ì´í„° ìƒíƒœ
                result_text += f"### ğŸ“‹ ê³µí†µ ë©”íƒ€ë°ì´í„° ìƒíƒœ\n"
                try:
                    stats = common_metadata_manager.get_statistics(db_sid)
                    result_text += f"- **ê³µí†µ ì¹¼ëŸ¼**: {stats['common_column_count']}ê°œ\n"
                    result_text += f"- **ì½”ë“œ ì¹¼ëŸ¼**: {stats['code_column_count']}ê°œ\n"
                    result_text += f"- **ì „ì²´ ì½”ë“œ ê°’**: {stats['total_code_count']}ê°œ\n"

                    if stats['common_column_count'] > 0:
                        result_text += f"- **ìƒíƒœ**: âœ… ì„¤ì •ë¨\n"
                    else:
                        result_text += f"- **ìƒíƒœ**: âš ï¸ ë¯¸ì„¤ì •\n"
                except Exception:
                    result_text += f"- **ìƒíƒœ**: âš ï¸ ë¯¸ì„¤ì •\n"
                result_text += "\n"

                # 5. í†µí•© ë©”íƒ€ë°ì´í„° ìƒíƒœ
                result_text += f"### ğŸ—‚ï¸ í†µí•© ë©”íƒ€ë°ì´í„° ìƒíƒœ\n"
                metadata_dir = Path("./metadata") / db_sid
                if metadata_dir.exists():
                    schema_dirs = [d for d in metadata_dir.iterdir() if d.is_dir()]
                    total_tables = 0
                    schema_info = []

                    for schema_dir in schema_dirs:
                        table_dirs = [d for d in schema_dir.iterdir() if d.is_dir()]
                        table_count = len(table_dirs)
                        total_tables += table_count
                        if table_count > 0:
                            schema_info.append(f"{schema_dir.name} ({table_count}ê°œ)")

                    if total_tables > 0:
                        result_text += f"- **ìƒì„±ëœ ë©”íƒ€ë°ì´í„°**: âœ… {total_tables}ê°œ í…Œì´ë¸”\n"
                        result_text += f"- **ìŠ¤í‚¤ë§ˆë³„**:\n"
                        for info in schema_info[:5]:
                            result_text += f"  - {info}\n"
                        if len(schema_info) > 5:
                            result_text += f"  - ... ì™¸ {len(schema_info) - 5}ê°œ\n"
                    else:
                        result_text += f"- **ìƒì„±ëœ ë©”íƒ€ë°ì´í„°**: âš ï¸ ì—†ìŒ\n"
                else:
                    result_text += f"- **ìƒì„±ëœ ë©”íƒ€ë°ì´í„°**: âš ï¸ ì—†ìŒ\n"
                result_text += "\n"

                # 6. CSV íŒŒì¼ ìƒíƒœ
                result_text += f"### ğŸ“„ CSV íŒŒì¼ ìƒíƒœ\n"
                common_metadata_dir = Path("./common_metadata") / db_sid
                csv_files = []
                if common_metadata_dir.exists():
                    if (common_metadata_dir / "common_columns.json").exists():
                        csv_files.append("âœ… ê³µí†µ ì¹¼ëŸ¼ ë¡œë“œë¨")
                    else:
                        csv_files.append("âš ï¸ ê³µí†µ ì¹¼ëŸ¼ ë¯¸ë¡œë“œ")

                    if (common_metadata_dir / "code_definitions.json").exists():
                        csv_files.append("âœ… ì½”ë“œ ì •ì˜ ë¡œë“œë¨")
                    else:
                        csv_files.append("âš ï¸ ì½”ë“œ ì •ì˜ ë¯¸ë¡œë“œ")

                    # ìŠ¤í‚¤ë§ˆë³„ í…Œì´ë¸” ì •ë³´ í™•ì¸
                    schema_files = list(common_metadata_dir.glob("*/table_info.json"))
                    if schema_files:
                        csv_files.append(f"âœ… í…Œì´ë¸” ì •ë³´ ({len(schema_files)}ê°œ ìŠ¤í‚¤ë§ˆ)")
                    else:
                        csv_files.append("âš ï¸ í…Œì´ë¸” ì •ë³´ ë¯¸ë¡œë“œ")
                else:
                    csv_files.append("âš ï¸ ê³µí†µ ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ")

                for file_status in csv_files:
                    result_text += f"- {file_status}\n"
                result_text += "\n"

            except Exception as e:
                result_text += f"âŒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n\n"

            result_text += "=" * 60 + "\n\n"

        result_text += "\n**ğŸ“Œ ì°¸ê³ ì‚¬í•­**:\n"
        result_text += "- ê³µí†µ ë©”íƒ€ë°ì´í„°: `import_common_columns_csv`, `import_code_definitions_csv`ë¡œ ì„¤ì •\n"
        result_text += "- í†µí•© ë©”íƒ€ë°ì´í„°: `extract_and_integrate_metadata`ë¡œ ìƒì„±\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        logger.error(f"ì—°ê²° ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        return [{
            "type": "text",
            "text": f"âŒ ì—°ê²° ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
        }]


# ============================================
# Tool 4: ìŠ¤í‚¤ë§ˆ ëª©ë¡
# ============================================

async def show_schemas(database_sid: str) -> list[dict]:
    """íŠ¹ì • DBì˜ ëª¨ë“  ìŠ¤í‚¤ë§ˆ ëª©ë¡"""
    try:
        connector = get_connector(database_sid)
        schemas = connector.list_schemas()

        result_text = f"ğŸ“‚ {database_sid}ì˜ ìŠ¤í‚¤ë§ˆ ëª©ë¡ ({len(schemas)}ê°œ)\n\n"
        for schema in schemas:
            result_text += f"- {schema}\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 5: í…Œì´ë¸” ëª©ë¡
# ============================================

async def show_tables(database_sid: str, schema_name: str, table_filter: str = None) -> list[dict]:
    """
    íŠ¹ì • ìŠ¤í‚¤ë§ˆì˜ í…Œì´ë¸” ëª©ë¡

    Args:
        database_sid: Database SID
        schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„
        table_filter: í…Œì´ë¸” ì´ë¦„ í•„í„° (LIKE íŒ¨í„´, ì˜ˆ: 'ISYS_%', '%_MASTER')
    """
    try:
        connector = get_connector(database_sid)
        tables = connector.list_tables(schema_name, table_filter)

        if table_filter:
            result_text = f"ğŸ“‹ {database_sid}.{schema_name}ì˜ í…Œì´ë¸” ëª©ë¡ (í•„í„°: {table_filter}) ({len(tables)}ê°œ)\n\n"
        else:
            result_text = f"ğŸ“‹ {database_sid}.{schema_name}ì˜ í…Œì´ë¸” ëª©ë¡ ({len(tables)}ê°œ)\n\n"

        for table in tables:
            result_text += f"- {table['TABLE_NAME']}"
            if table.get('NUM_ROWS'):
                result_text += f" ({table['NUM_ROWS']:,}ê°œ í–‰)"
            result_text += "\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 6: í…Œì´ë¸” êµ¬ì¡° ìƒì„¸
# ============================================

async def describe_table(
    database_sid: str,
    schema_name: str,
    table_name: str
) -> list[dict]:
    """í…Œì´ë¸” êµ¬ì¡° ìƒì„¸ ì¡°íšŒ"""
    try:
        connector = get_connector(database_sid)

        # ì¹¼ëŸ¼ ì •ë³´
        columns = connector.extract_table_columns(schema_name, table_name)
        primary_keys = connector.extract_primary_keys(schema_name, table_name)
        foreign_keys = connector.extract_foreign_keys(schema_name, table_name)
        indexes = connector.extract_indexes(schema_name, table_name)
        comment = connector.get_table_comment(schema_name, table_name)

        result_text = f"ğŸ“Š í…Œì´ë¸” êµ¬ì¡°: {database_sid}.{schema_name}.{table_name}\n\n"

        if comment:
            result_text += f"ì„¤ëª…: {comment}\n\n"

        result_text += "## ì¹¼ëŸ¼\n\n"
        for col in columns:
            pk_mark = " [PK]" if col['COLUMN_NAME'] in primary_keys else ""
            nullable = "NULL" if col['NULLABLE'] == 'Y' else "NOT NULL"

            result_text += f"- {col['COLUMN_NAME']}{pk_mark}\n"
            result_text += f"  íƒ€ì…: {col['DATA_TYPE']}, {nullable}\n"
            if col.get('COMMENTS'):
                result_text += f"  ì„¤ëª…: {col['COMMENTS']}\n"
            result_text += "\n"

        if foreign_keys:
            result_text += "\n## Foreign Keys\n\n"
            for fk in foreign_keys:
                result_text += f"- {fk['COLUMN_NAME']} â†’ {fk['REF_TABLE']}.{fk['REF_COLUMN']}\n"

        if indexes:
            result_text += "\n## Indexes\n\n"
            for idx in indexes:
                result_text += f"- {idx['INDEX_NAME']} ({idx['UNIQUENESS']}): {idx['COLUMNS']}\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 7: í”„ë¡œì‹œì €/í•¨ìˆ˜ ëª©ë¡
# ============================================

async def show_procedures(
    database_sid: str,
    schema_name: str
) -> list[dict]:
    """í”„ë¡œì‹œì € ë° í•¨ìˆ˜ ëª©ë¡"""
    try:
        connector = get_connector(database_sid)
        procedures = connector.list_procedures(schema_name)

        result_text = f"âš™ï¸ {database_sid}.{schema_name}ì˜ í”„ë¡œì‹œì €/í•¨ìˆ˜ ({len(procedures)}ê°œ)\n\n"

        for proc in procedures:
            result_text += f"- {proc['OBJECT_NAME']} ({proc['OBJECT_TYPE']})\n"
            result_text += f"  ìƒíƒœ: {proc['STATUS']}, ìˆ˜ì •ì¼: {proc['LAST_DDL_TIME']}\n\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 8: í”„ë¡œì‹œì € ì†ŒìŠ¤ ì½”ë“œ
# ============================================

async def show_procedure_source(
    database_sid: str,
    schema_name: str,
    procedure_name: str
) -> list[dict]:
    """í”„ë¡œì‹œì €/í•¨ìˆ˜ ì†ŒìŠ¤ ì½”ë“œ"""
    try:
        connector = get_connector(database_sid)
        source = connector.get_procedure_source(schema_name, procedure_name)

        if not source:
            return [{
                "type": "text",
                "text": f"í”„ë¡œì‹œì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {procedure_name}"
            }]

        result_text = f"ğŸ“„ í”„ë¡œì‹œì € ì†ŒìŠ¤: {database_sid}.{schema_name}.{procedure_name}\n\n"
        result_text += "```sql\n"
        result_text += source
        result_text += "\n```"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# Tool 9: SQL ì§ì ‘ ì‹¤í–‰
# ============================================

async def execute_sql(
    database_sid: str,
    sql: str,
    max_rows: int = 1000
) -> list[dict]:
    """SQL ì¿¼ë¦¬ ì§ì ‘ ì‹¤í–‰ (SELECTë§Œ)"""
    try:
        connector = get_connector(database_sid)
        executor = SQLExecutor(connector)

        result = executor.execute_select(sql, max_rows)

        if result['status'] == 'error':
            return [{
                "type": "text",
                "text": f"âŒ {result['message']}"
            }]

        result_text = f"âœ… ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ\n\n"
        result_text += f"SQL:\n```sql\n{sql}\n```\n\n"
        result_text += f"ê²°ê³¼: {result['row_count']}ê°œ í–‰\n\n"

        # ê²°ê³¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ
        if result['rows']:
            import json
            result_text += "```json\n"
            result_text += json.dumps(result['rows'][:10], ensure_ascii=False, indent=2)
            result_text += "\n```"

            if len(result['rows']) > 10:
                result_text += f"\n\n... ì™¸ {len(result['rows']) - 10}ê°œ í–‰"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ SQL ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 10: ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ìœ„í•œ í…Œì´ë¸” ìš”ì•½ ì œê³µ (Stage 1)
# ============================================

async def get_table_summaries_for_query(
    database_sid: str,
    schema_name: str,
    natural_query: str = ""
) -> list[dict]:
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ìœ„í•œ í…Œì´ë¸” ìš”ì•½ ì •ë³´ ì œê³µ (Stage 1)

    Claudeê°€ ì´ ì •ë³´ë¥¼ ë³´ê³  ê´€ë ¨ í…Œì´ë¸”ì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    """
    try:
        summaries = metadata_manager.load_table_summaries(database_sid, schema_name)

        import json
        result_text = f"ğŸ“Š í…Œì´ë¸” ìš”ì•½ ì •ë³´ (Stage 1)\n\n"
        result_text += f"**ì§ˆë¬¸**: {natural_query}\n\n"
        result_text += f"**Database**: {database_sid}\n"
        result_text += f"**Schema**: {schema_name}\n\n"
        result_text += "**í…Œì´ë¸” ëª©ë¡**:\n\n"

        for table_name, summary in summaries.items():
            result_text += f"### {table_name}\n"
            result_text += f"- **ëª©ì **: {summary.get('business_purpose', 'N/A')}\n"
            result_text += f"- **ì¹¼ëŸ¼ ìˆ˜**: {summary.get('column_count', 0)}\n"
            result_text += f"- **ì£¼ìš” ì¹¼ëŸ¼**: {', '.join(summary.get('key_columns', []))}\n"
            result_text += f"- **ì—°ê´€ í…Œì´ë¸”**: {', '.join(summary.get('related_tables', []))}\n\n"

        result_text += "\n---\n\n"
        result_text += "**ë‹¤ìŒ ë‹¨ê³„**: ìœ„ í…Œì´ë¸”ë“¤ ì¤‘ì—ì„œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ í…Œì´ë¸”(ìµœëŒ€ 5ê°œ)ì„ ì„ íƒí•˜ê³ ,\n"
        result_text += "`get_detailed_metadata_for_sql` Toolì„ í˜¸ì¶œí•˜ì—¬ ìƒì„¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë°›ì•„ SQLì„ ìƒì„±í•˜ì„¸ìš”.\n"

        return [{"type": "text", "text": result_text}]

    except FileNotFoundError:
        return [{
            "type": "text",
            "text": f"âŒ í…Œì´ë¸” ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {database_sid}.{schema_name}\në©”íƒ€ë°ì´í„°ë¥¼ ë¨¼ì € ì¶”ì¶œí•´ì£¼ì„¸ìš”."
        }]
    except Exception as e:
        logger.error(f"í…Œì´ë¸” ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ í…Œì´ë¸” ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }]


# ============================================
# Tool 11: ì„ íƒëœ í…Œì´ë¸”ë“¤ì˜ ìƒì„¸ ë©”íƒ€ë°ì´í„° ì œê³µ (Stage 2)
# ============================================

async def get_detailed_metadata_for_sql(
    database_sid: str,
    schema_name: str,
    table_names: str,  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í…Œì´ë¸”ëª…
    natural_query: str = ""
) -> list[dict]:
    """
    ì„ íƒëœ í…Œì´ë¸”ë“¤ì˜ ìƒì„¸ ë©”íƒ€ë°ì´í„° ì œê³µ (Stage 2)

    Claudeê°€ ì´ ì •ë³´ë¥¼ ë³´ê³  ì •í™•í•œ SQLì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    """
    try:
        # í…Œì´ë¸”ëª… íŒŒì‹±
        selected_tables = [t.strip() for t in table_names.split(',')]

        if len(selected_tables) > 5:
            return [{
                "type": "text",
                "text": f"âš ï¸ í…Œì´ë¸”ì€ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬: {len(selected_tables)}ê°œ)"
            }]

        import json
        result_text = f"ğŸ“Š ìƒì„¸ ë©”íƒ€ë°ì´í„° (Stage 2)\n\n"
        result_text += f"**ì§ˆë¬¸**: {natural_query}\n\n"
        result_text += f"**ì„ íƒëœ í…Œì´ë¸”**: {', '.join(selected_tables)}\n\n"
        result_text += "---\n\n"

        # ê° í…Œì´ë¸”ì˜ ìƒì„¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        all_metadata = []
        for table_name in selected_tables:
            try:
                metadata = metadata_manager.load_unified_metadata(
                    database_sid, schema_name, table_name
                )
                all_metadata.append(metadata)

                # ê°„ë‹¨í•œ ìš”ì•½ í‘œì‹œ
                result_text += f"### {table_name}\n"
                result_text += f"- ëª©ì : {metadata.get('table_info', {}).get('business_purpose', 'N/A')}\n"
                result_text += f"- ì¹¼ëŸ¼ ìˆ˜: {len(metadata.get('columns', []))}\n\n"

            except FileNotFoundError:
                result_text += f"### {table_name}\n"
                result_text += f"âš ï¸ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        # ì „ì²´ ë©”íƒ€ë°ì´í„° JSON ì œê³µ
        result_text += "\n---\n\n"
        result_text += "**ì „ì²´ ë©”íƒ€ë°ì´í„° (SQL ìƒì„±ìš©)**:\n\n"
        result_text += "```json\n"
        result_text += json.dumps(all_metadata, ensure_ascii=False, indent=2)
        result_text += "\n```\n\n"

        result_text += "---\n\n"
        result_text += "**ë‹¤ìŒ ë‹¨ê³„**: ìœ„ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ Oracle SQLì„ ìƒì„±í•œ í›„,\n"
        result_text += "`execute_sql` Toolì„ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\n\n"
        result_text += "**Oracle SQL ìƒì„± ê°€ì´ë“œ**:\n"
        result_text += "- Schema.Table í˜•ì‹ ì‚¬ìš© (ì˜ˆ: SCOTT.ORDERS)\n"
        result_text += "- Oracle ë‚ ì§œ í•¨ìˆ˜ ì‚¬ìš© (TRUNC, ADD_MONTHS, TO_CHAR ë“±)\n"
        result_text += "- ì½”ë“œ ì¹¼ëŸ¼ì˜ ê²½ìš° ì½”ë“œê°’ìœ¼ë¡œ WHERE ì¡°ê±´ ì‘ì„±\n"
        result_text += "- FK ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ JOIN ì¡°ê±´ ì‘ì„±\n"

        return [{"type": "text", "text": result_text}]

    except Exception as e:
        logger.error(f"ìƒì„¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return [{
            "type": "text",
            "text": f"âŒ ìƒì„¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        }]




# ============================================
# Tool 12: í…Œì´ë¸” ë©”íƒ€ì •ë³´ ì¡°íšŒ
# ============================================

async def get_table_metadata(
    database_sid: str,
    schema_name: str,
    table_name: str
) -> list[dict]:
    """í†µí•© ë©”íƒ€ì •ë³´ ì¡°íšŒ"""
    try:
        metadata = metadata_manager.load_unified_metadata(
            database_sid, schema_name, table_name
        )

        import json
        result_text = f"ğŸ“Š í†µí•© ë©”íƒ€ì •ë³´: {database_sid}.{schema_name}.{table_name}\n\n"
        result_text += "```json\n"
        result_text += json.dumps(metadata, ensure_ascii=False, indent=2)
        result_text += "\n```"

        return [{"type": "text", "text": result_text}]

    except FileNotFoundError:
        return [{
            "type": "text",
            "text": f"âŒ ë©”íƒ€ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {database_sid}.{schema_name}.{table_name}"
        }]
    except Exception as e:
        return [{
            "type": "text",
            "text": f"âŒ ì—ëŸ¬: {str(e)}"
        }]


# ============================================
# ì„œë²„ ì‹¤í–‰
# ============================================
async def main():
    """MCP ì„œë²„ ì‹¤í–‰"""
    logger.info("="*60)
    logger.info("ğŸš€ Oracle Database MCP ì„œë²„ ì‹œì‘")
    logger.info("="*60)

    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options()
        )


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
